{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormatCompiler():\n",
    "    \n",
    "    def __init__(self, dataset_name = None, part = None, \n",
    "                 input_dir = '/home/biodb/data/dataset_collection/datasets/3_standard_dataset',\n",
    "                 output_dir = '/home/biodb/data/dataset_collection/datasets/genentech_format_copy'):\n",
    "        \n",
    "        self.data_name = dataset_name\n",
    "        self.part = part\n",
    "        self.dataset_name = dataset_name + '/' + part\n",
    "        self.new_name = dataset_name + '_' + part\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.unstructured_data_dir =  self.input_dir + '/' + self.dataset_name + '/processed_data' + '/unstructuredData.json'\n",
    "        self.cell_anno_dir = self.input_dir + '/' + self.dataset_name + '/processed_data' + '/cellAnnotation.tsv'\n",
    "        self.gene_anno_dir = self.input_dir + '/' + self.dataset_name + '/processed_data' + '/geneAnnotation.tsv'\n",
    "        self.exp_raw_dir = self.input_dir + '/' + self.dataset_name + '/processed_data' + '/expressionMatrix_rawCounts.tsv'\n",
    "        self.exp_tpm_dir = self.input_dir + '/' + self.dataset_name + '/processed_data' + '/expressionMatrix_TPM.tsv'\n",
    "        self.exp_normalized_dir = self.input_dir + '/' + self.dataset_name + '/processed_data' + '/expressionMatrix_normalized.tsv'\n",
    "        \n",
    "        self.metadata_dir = self.output_dir + '/' + self.dataset_name + '/metadata.tsv'\n",
    "        self.phenotype_dir = self.output_dir + '/' + self.dataset_name + '/phenotype.tsv'\n",
    "        self.geneID_dir = self.output_dir + '/' + self.dataset_name + '/geneID.tsv'\n",
    "        self.cellID_dir = self.output_dir + '/' + self.dataset_name + '/cellID.tsv'\n",
    "        self.marker_genes_dir = self.output_dir + '/' + self.dataset_name + '/markerGenes.tsv'\n",
    "        self.ontology_dir =  self.output_dir + '/' + self.dataset_name + '/ontologyMapping.tsv'\n",
    "    \n",
    "    def making_dir(self):\n",
    "        path = self.output_dir + '/' + self.data_name\n",
    "        if not self.data_name in os.listdir(self.output_dir):\n",
    "            os.makedirs(path)\n",
    "        elif not self.part in os.listdir(path):\n",
    "            os.makedirs(self.output_dir + '/' + self.dataset_name)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def transform_format(self):\n",
    "        \n",
    "        self.transform_phenotype()\n",
    "        self.transform_marker_genes()\n",
    "        self.transform_ontology()\n",
    "        self.transform_geneID()\n",
    "        self.transform_metadata()\n",
    "        \n",
    "        return_message = 'successfully'\n",
    "        return return_message\n",
    "    \n",
    "    def transform_metadata(self):\n",
    "        \n",
    "        self.making_dir()\n",
    "        with open(self.unstructured_data_dir, 'r') as json_file:\n",
    "            meta = json.load(json_file)['metadata']\n",
    "    \n",
    "        with open(self.exp_normalized_dir, 'r') as file:\n",
    "            x = file.readline()\n",
    "            x = file.readline()\n",
    "            normalized_method = x.split('\\t')[0]\n",
    "        \n",
    "        metadata = dict()\n",
    "        list_names = ['datasetID','title','accessionNumber','abstract','source','sourceID','numberOfCells','libraryPreparationMethod',\n",
    "                      'sequencingPlatform','pubmedID','clusteringMethod','biomarkerDerivationMethod','fastqURL','genomeBuild','annotation',\n",
    "                      'subDataset','description']\n",
    "        for i in list_names:\n",
    "            metadata[i] = meta[i]\n",
    "        metadata['datasetID'] = self.new_name\n",
    "        metadata['normalizationMethod'] = normalized_method\n",
    "        metadata = pd.DataFrame(metadata, index = [0])\n",
    "        metadata.replace('notAvailable','',inplace = True)\n",
    "        metadata.replace('NA','',inplace = True)\n",
    "        metadata = metadata.fillna('')\n",
    "        metadata.to_csv(self.metadata_dir ,sep = '\\t',index = False)\n",
    "            \n",
    "        return_message = 'successfully generated Metadata.tsv'\n",
    "        return return_message\n",
    "    \n",
    "    def transform_phenotype(self):\n",
    "        \n",
    "        self.making_dir()\n",
    "        cell_anno = pd.read_csv(self.cell_anno_dir ,sep='\\t')\n",
    "        df_cell = cell_anno.drop(['cellOntologyName','cellOntologyID','FACSMarker'], axis=1)\n",
    "        df_cell.replace('notAvailable','',inplace = True)\n",
    "        df_cell['filtered'] = True\n",
    "        df_cell = df_cell.fillna('')\n",
    "        cell_raw = pd.read_csv(self.exp_raw_dir, sep='\\t')['cellID'].tolist()\n",
    "        if cell_raw != []:\n",
    "            if len(cell_raw) != cell_anno.shape[0]:\n",
    "                cell = pd.DataFrame()\n",
    "                cell['cellID'] = cell_raw\n",
    "                df_cell = cell.merge(df_cell, how = 'left', sort = False)\n",
    "                df_cell['filtered'] = df_cell['filtered'].fillna(False).tolist()\n",
    "            else:\n",
    "                df_cell['filtered'] = False\n",
    "        df_cell = df_cell.fillna('')\n",
    "        df_cell.to_csv(self.phenotype_dir ,sep = '\\t',index = False)\n",
    "        return_message = 'successfully generated phenotype.tsv'\n",
    "        return return_message\n",
    "    \n",
    "    def transform_marker_genes(self):\n",
    "        \n",
    "        self.making_dir()\n",
    "        with open(self.unstructured_data_dir, 'r') as json_file:\n",
    "            markers= json.load(json_file)['markerGenes']\n",
    "        if markers == {}:\n",
    "            pass \n",
    "        else:\n",
    "            cluster = [x for x in markers]\n",
    "            marker_genes = pd.DataFrame(markers[cluster[0]])\n",
    "            marker_genes['clusterName'] = cluster[0]\n",
    "            for x in range(1,len(cluster)):\n",
    "                z = pd.DataFrame(markers[cluster[x]])\n",
    "                z['clusterName'] = cluster[x]\n",
    "                marker_genes = marker_genes.append(z, ignore_index = True)\n",
    "            \n",
    "            marker_genes.replace('notAvailable','',inplace = True)\n",
    "            marker_genes.to_csv(self.marker_genes_dir, sep = '\\t', index = False)   \n",
    "\n",
    "            return_message = 'successfully generated Marker_genes.tsv'\n",
    "            return return_message\n",
    "    \n",
    "    def generate_ensemblID(self, gene_symbol):\n",
    "        \n",
    "        self.making_dir()\n",
    "        ref_dict = {}\n",
    "        with open(self.unstructured_data_dir, 'r') as json_file:\n",
    "            metadata = json.load(json_file)['metadata']\n",
    "        taxonomyID = metadata['taxonomyID']\n",
    "        gene_ref_dir = '/home/biodb/data/abio_database_pipeline/gene_references'\n",
    "        if not taxonomyID == 0:\n",
    "            species = taxonomyID\n",
    "        if species == 9606:\n",
    "            df = pd.read_csv(gene_ref_dir + '/human_unique_id_length.tsv', sep='\\t')\n",
    "            for i, symbol in enumerate(df['Gene_name'].tolist()):\n",
    "                ref_dict[symbol] = df['Gene_ID'].tolist()[i]\n",
    "        \n",
    "        elif species == 10090:\n",
    "            df = pd.read_csv(gene_ref_dir + '/mouse_unique_id_length.tsv', sep='\\t')\n",
    "            for i, symbol in enumerate(df['Gene_name'].tolist()):\n",
    "                ref_dict[symbol] = df['Gene_ID'].tolist()[i]\n",
    "        \n",
    "        elif species == 7955:\n",
    "            df = pd.read_csv(gene_ref_dir + '/zebra_fish_unique_id_length.tsv', sep='\\t')\n",
    "            for i, symbol in enumerate(df['Gene_name'].tolist()):\n",
    "                ref_dict[symbol] = df['Gene_ID'].tolist()[i]\n",
    "        IDs = []\n",
    "        for i, name in enumerate(gene_symbol):\n",
    "            try:\n",
    "                IDs.append(ref_dict[name])\n",
    "            except:\n",
    "                IDs.append('notAvailable')\n",
    "        return IDs\n",
    "        \n",
    "    def transform_geneID(self):\n",
    "        \n",
    "        self.making_dir()\n",
    "        gene_anno = pd.read_csv(self.gene_anno_dir, sep = '\\t')\n",
    "        genes = pd.DataFrame(gene_anno.iloc[:,:2])\n",
    "        genes.replace('notAvailable' or 'NA','',inplace = True)\n",
    "        genes['filtered'] = False\n",
    "        with open(self.exp_raw_dir, 'r') as file:\n",
    "            x = file.readline()[:-1]\n",
    "        gene_raw = x.split('\\t')[1:]\n",
    "        with open(self.exp_normalized_dir, 'r') as file:\n",
    "            y = file.readline()[:-1]\n",
    "        gene_norm = y.split('\\t')[2:]\n",
    "        if gene_norm[0] == 'geneSymbol1':\n",
    "            pass\n",
    "        elif gene_raw[0] == 'geneSymbol1':\n",
    "            genes['filtered'] = True\n",
    "        elif len(gene_raw) != genes.shape[0]:\n",
    "            filtered = [x in gene_norm for x in gene_raw]\n",
    "            genes = pd.DataFrame()\n",
    "            genes['geneSymbol'] = gene_raw\n",
    "            genes['ensemblID'] = self.generate_ensemblID(gene_raw)\n",
    "            genes['filered'] = filtered\n",
    "        elif len(gene_raw) == genes.shape[0]:\n",
    "            genes['filtered'] = True\n",
    "        \n",
    "        genes.to_csv(self.geneID_dir ,sep = '\\t',index = False)\n",
    "        \n",
    "        return_message = 'successfully generated geneID.tsv'\n",
    "        return return_message\n",
    "        \n",
    "        \n",
    "    def transform_ontology(self):\n",
    "        \n",
    "        self.making_dir()\n",
    "        cell_anno = pd.read_csv(self.cell_anno_dir ,sep='\\t')\n",
    "        onto = cell_anno[['clusterID','clusterName','cellOntologyName','cellOntologyID']]\n",
    "        ontology = onto.drop_duplicates()\n",
    "        ontology.replace('notAvailable','',inplace = True)\n",
    "        ontology = ontology.fillna('')\n",
    "        if not set(ontology['cellOntologyName']) == {''}:\n",
    "            ontology.to_csv(self.ontology_dir, sep = '\\t',index = False)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return_message = 'successfully generated Ontology_mapping.tsv'\n",
    "        return return_message\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,33):\n",
    "    p = 'part_' + str(i)\n",
    "    myformatcomplier = FormatCompiler(dataset_name='No_17', part = p)\n",
    "    myformatcomplier.transform_marker_genes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'successfully generated Metadata.tsv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myformatcomplier = FormatCompiler(dataset_name='No_5', part = 'part_2')\n",
    "myformatcomplier.transform_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No_33/part_1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myformatcomplier.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_anno = pd.read_csv(myformatcomplier.cell_anno_dir ,sep='\\t')\n",
    "df_cell = cell_anno.drop(['cellOntologyName','cellOntologyID','FACSMarker'], axis=1)\n",
    "df_cell.replace('notAvailable','',inplace = True)\n",
    "df_cell['filtered'] = False\n",
    "df_cell.to_csv(myformatcomplier.phenotype_dir ,sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = '/home/biodb/data/dataset_collection/datasets/3_standard_dataset/'\n",
    "for i in [43]:\n",
    "    dataset_name = 'No_' + str(i)\n",
    "    path1 = path + dataset_name\n",
    "    for j in os.listdir(path1):\n",
    "        try:\n",
    "            myformatcomplier = FormatCompiler(dataset_name, part = j, input_dir = path)\n",
    "            myformatcomplier.transform_format()\n",
    "        except:\n",
    "            print([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "path = '/home/biodb/data/dataset_collection/datasets/3_standard_dataset/'\n",
    "for i in range(46,47):\n",
    "    path1 = path + 'No_' + str(i)\n",
    "    for j in os.listdir(path1):\n",
    "        path2 = path1 + '/' + str(j) + '/processed_data/cellAnnotation.tsv'\n",
    "        cell = pd.read_csv(path2,sep='\\t')\n",
    "        delete = ['libraryPreparationMethod','sequencingPlatform','PCA1','PCA2']\n",
    "        for i in delete:\n",
    "            if i in cell.columns.tolist():\n",
    "                cell = cell.drop(columns=[i])\n",
    "        cell.to_csv(path2, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 'part_1']\n",
      "[34, 'part_1']\n",
      "[35, 'part_1']\n",
      "[36, 'part_1']\n",
      "[37, 'part_1']\n",
      "[38, 'part_2']\n",
      "[38, 'part_1']\n",
      "[39, 'part_1']\n",
      "[40, 'part_1']\n",
      "[40, 'part_2']\n",
      "[41, 'part_1']\n",
      "[41, 'part_2']\n",
      "[41, 'part_3']\n",
      "[42, 'part_2']\n",
      "[42, 'part_1']\n",
      "[42, 'part_3']\n",
      "[42, 'part_4']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "path = '/home/biodb/data/dataset_collection/datasets/3_standard_dataset/'\n",
    "for i in range(33,43):\n",
    "    path1 = path + 'No_' + str(i)\n",
    "    for j in os.listdir(path1):\n",
    "        path2 = path1 + '/' + str(j) + '/processed_data/unstructuredData.json'\n",
    "        with open(path2,'r') as file:\n",
    "            uns = json.load(file)\n",
    "            meta = uns['metadata']\n",
    "        if 'description' in meta.keys():\n",
    "            print([i,j])\n",
    "        else:\n",
    "            meta['description'] = ''\n",
    "            uns['metadata'] = meta\n",
    "            with open(path2,'w') as file:\n",
    "                json.dump(uns,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No_5', '.ipynb_checkpoints', 'meta']\n",
      "['No_5', '.ipynb_checkpoints', 'onto']\n",
      "['No_6', 'part_1', 'onto']\n",
      "['No_12', '.ipynb_checkpoints', 'meta']\n",
      "['No_12', '.ipynb_checkpoints', 'onto']\n",
      "['No_12', 'part_1', 'onto']\n",
      "['No_14', 'part_2', 'onto']\n",
      "['No_14', 'part_1', 'onto']\n",
      "['No_15', 'part_3', 'onto']\n",
      "['No_15', 'part_2', 'onto']\n",
      "['No_15', 'part_1', 'onto']\n",
      "['No_20', 'part_1', 'onto']\n",
      "['No_21', 'part_1', 'onto']\n",
      "['No_22', 'part_1', 'onto']\n",
      "['No_23', 'part_1', 'onto']\n",
      "['No_28', 'part_1', 'onto']\n",
      "['No_32', 'part_2', 'onto']\n",
      "['No_34', 'part_1', 'onto']\n",
      "['No_36', 'part_1', 'onto']\n",
      "['No_39', '.ipynb_checkpoints', 'meta']\n",
      "['No_39', '.ipynb_checkpoints', 'onto']\n",
      "['No_45', 'part_3', 'onto']\n",
      "['No_45', 'part_4', 'onto']\n",
      "['No_45', 'part_2', 'onto']\n",
      "['No_47', 'part_1', 'onto']\n",
      "['No_47', 'part_2', 'onto']\n",
      "['No_47', 'part_4', 'onto']\n",
      "['No_47', 'part_3', 'onto']\n",
      "['No_48', 'part_3', 'onto']\n",
      "['No_49', '.ipynb_checkpoints', 'meta']\n",
      "['No_49', '.ipynb_checkpoints', 'onto']\n",
      "['No_49', 'part_1', 'onto']\n",
      "['No_50', '.ipynb_checkpoints', 'meta']\n",
      "['No_50', '.ipynb_checkpoints', 'onto']\n",
      "['No_50', 'part_1', 'onto']\n",
      "['No_53', '.ipynb_checkpoints', 'meta']\n",
      "['No_53', '.ipynb_checkpoints', 'onto']\n",
      "['No_59', '.ipynb_checkpoints', 'meta']\n",
      "['No_59', '.ipynb_checkpoints', 'onto']\n",
      "['No_63', '.ipynb_checkpoints', 'meta']\n",
      "['No_63', '.ipynb_checkpoints', 'onto']\n",
      "['No_63', 'part_1', 'onto']\n",
      "['No_64', '.ipynb_checkpoints', 'meta']\n",
      "['No_64', '.ipynb_checkpoints', 'onto']\n",
      "['No_67', 'part_1', 'onto']\n"
     ]
    }
   ],
   "source": [
    "onto = pd.DataFrame()\n",
    "dirs = '/home/biodb/data/dataset_collection/datasets/4_genentech_format/ready/'\n",
    "for i in range(1,69):\n",
    "    k = 'No_' + str(i)\n",
    "    paths = dirs + k + '/'\n",
    "    for j in os.listdir(paths):\n",
    "        try:\n",
    "            path = paths + '/' + j\n",
    "            meta = pd.read_csv(path + '/metadata.tsv', sep = '\\t')\n",
    "            meta['datasetID'] = k + '_' + j\n",
    "            meta.to_csv(path + '/metadata.tsv', sep = '\\t', index=False)\n",
    "        except:\n",
    "            print([k,j,'meta'])\n",
    "        try:\n",
    "            ot = pd.read_csv(path + '/ontologyMapping.tsv', sep = '\\t')\n",
    "            ot.insert(0,'datasetID',k)\n",
    "            ot.drop(columns=['clusterID'],inplace=True)\n",
    "            onto = onto.append(ot)\n",
    "        except:\n",
    "            print([k,j,'onto'])\n",
    "onto = onto.fillna('')\n",
    "onto = onto.drop_duplicates()\n",
    "onto.to_csv('/home/biodb/data/dataset_collection/datasets/4_genentech_format/ontologyMapping_collection.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No_27', 0    GSE81608\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_28', 0    GSE86618\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_29', 0    GSE84465\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_30', 0    GSE99795\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_31', 0    GSE89232\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_32', 0    GSE120506\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_32', 0    GSE112013\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_33', 0    GSE106510\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_34', 0    GSE103334\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_35', 0    GSE115978\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_35', 0    GSE115978\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_36', 0    GSE115469\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_37', 0    GSE106960\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_37', 0    GSE106960\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_37', 0    GSE106960\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_37', 0    GSE106960\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_38', 0    GSE128423\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_38', 0    GSE128423\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_39', 0    GSE110499\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_39', '.ipynb_checkpoints']\n",
      "['No_40', 0    GSE111831\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_41', 0    GSE84133\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_41', 0    GSE84133\n",
      "Name: accessionNumber, dtype: object]\n",
      "['No_42', 0    GSE107585\n",
      "Name: accessionNumber, dtype: object]\n"
     ]
    }
   ],
   "source": [
    "dirs = '/home/biodb/data/dataset_collection/datasets/4_genentech_format/ready/'\n",
    "dataset_list = pd.DataFrame()\n",
    "for i in range(27,43):\n",
    "    k = 'No_' + str(i)\n",
    "    paths = dirs + k \n",
    "    for j in os.listdir(paths):\n",
    "        try:\n",
    "            path = paths + '/' + j\n",
    "            meta = pd.read_csv(path + '/metadata.tsv', sep = '\\t')\n",
    "            meta1 = pd.DataFrame()\n",
    "            meta1['datasetID'] = k\n",
    "            meta1['accessionNumber'] = meta['accessionNumber']\n",
    "            meta1['title'] = meta['title']\n",
    "            dataset_list = dataset_list.append(meta1)\n",
    "            print([k,meta['accessionNumber']])\n",
    "        except:\n",
    "            print([k,j])\n",
    "#dataset_list = dataset_list.drop_duplicates()\n",
    "#dataset_list.to_csv('/home/biodb/data/dataset_collection/datasets/4_genentech_format/datasets_list.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No_42'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
